{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting ad positioning using Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placement of ads on website is the primary problem for companies that operate on ad revenue. The position where the ad is placed plays pivotal role on whether or not the ad will be clicked. Here we have the following choices:\n",
    "\t1. Place them randomly, or\n",
    "\t2. Place the ad on the same position\n",
    "\n",
    "The problem with placing the ad on the same position is the user, after a certain time, will start ignoring the space since he's used to seeing ad at the place, he will end up ignoring that particular position hereafter. Hence, this will reduce the number of clicks on ads. The problem with the former option, placing them randomly, is it wouldn't take optimal positions into consideration. For instance, text beside images are viewed higher number of times than those text which are placed at a distance. It is infeasible to go through every website and repeat the procedure. \n",
    "\n",
    "Solution: Reinforcement Learning\n",
    "Using Reinforcement Learning we can approximate the human behavior. \n",
    "\n",
    "Why Reinforcement Learning? \n",
    "We cannot use traditional Machine Learning here, since it requires:\n",
    "\t1. Huge data\n",
    "\t2. Features\n",
    "\t3. Tuning of many hyperparameters\n",
    "And we neither have huge data, nor features. The only data we have is the position of the baner/ad and whether or not it was clicked. We will use this dataset from Kaggle: https://www.kaggle.com/akram24/ads-ctr-optimisation. We will solve this problem using Q-Learning. The reason for using Q-Learning here is :\n",
    "\t1. It is model free, so it doesn't require to know all the states.\n",
    "\t2. Intuitive to understand, and converges faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our environment will be the dataset. It contains 10 ads position per row having values either 1, when the ad is clicked, or 0 when it is not. Every row can be considered as a state in the space, considering it kind of a navigation across multiple pages (on website, for instance) Lets load the dataset and visualize the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ad 1</th>\n",
       "      <th>Ad 2</th>\n",
       "      <th>Ad 3</th>\n",
       "      <th>Ad 4</th>\n",
       "      <th>Ad 5</th>\n",
       "      <th>Ad 6</th>\n",
       "      <th>Ad 7</th>\n",
       "      <th>Ad 8</th>\n",
       "      <th>Ad 9</th>\n",
       "      <th>Ad 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ad 1  Ad 2  Ad 3  Ad 4  Ad 5  Ad 6  Ad 7  Ad 8  Ad 9  Ad 10\n",
       "0     1     0     0     0     1     0     0     0     1      0\n",
       "1     0     0     0     0     0     0     0     0     1      0\n",
       "2     0     0     0     0     0     0     0     0     0      0\n",
       "3     0     1     0     0     0     0     0     1     0      0\n",
       "4     0     0     0     0     0     0     0     0     0      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = pd.read_csv('Ads_CTR_Optimisation.csv')\n",
    "env.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random policy\n",
    "\n",
    "If we were to not have Q-Learning, we would place the ads randomly at given positions. We will now simulate the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward collected: 1245\n"
     ]
    }
   ],
   "source": [
    "# total rewards earned\n",
    "reward = 0\n",
    "# random policy: for every state, choose a random\n",
    "# position for displaying the ad\n",
    "for x in range(len(env)):\n",
    "    action = np.random.randint(0, 10)\n",
    "    # if the guess was correct, increase the reward\n",
    "    if env.values[x][action] == 1:\n",
    "        reward += 1\n",
    "print(\"Reward collected: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Max Policy\n",
    "Another question we might ask, is to display the ad where it is clicked the most number of times. For instance, there might be a certain position where the ad clicked with a higher probability. Since the values of the rows is either 1 or 0, we can sum across the columns and count the number of times ad in the position was clicked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    counts\n",
       "ad        \n",
       "1     1703\n",
       "2     1295\n",
       "3      728\n",
       "4     1196\n",
       "5     2695\n",
       "6      126\n",
       "7     1112\n",
       "8     2091\n",
       "9      952\n",
       "10     489"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicked_counts = env.values.sum(axis=0)\n",
    "counts = pd.DataFrame({\"ad\": np.arange(1, 11), \"counts\": clicked_counts})\n",
    "counts.set_index(\"ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which indicates ad 5 was clicked 2695 times. So if we were to always place an ad on position 5, it would be click around 2695 times. But can we do better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Policy Iteration (Dynamic Programming) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes trained: 11\n"
     ]
    }
   ],
   "source": [
    "action_space = np.arange(0, 100)\n",
    "# starting with random policy, choose a random choice for\n",
    "# every state in the environment\n",
    "state_size = len(env)\n",
    "policy = [random.choice(action_space) for x in range(state_size)]\n",
    "# will take random action for the first time\n",
    "first_time = True\n",
    "small_change = 1e-20\n",
    "gamma = 0.9\n",
    "episodes = 0\n",
    "max_episodes = 10\n",
    "\n",
    "V = dict()\n",
    "# last positions reward will be 1\n",
    "V[10000] = 1\n",
    "\n",
    "# initially the value function for all states\n",
    "# will be random values close to zero\n",
    "for i in range(state_size):\n",
    "    V[i] = np.random.random()\n",
    "\n",
    "while episodes < max_episodes:\n",
    "    # policy evaluation\n",
    "    while True:\n",
    "        if episodes > max_episodes:\n",
    "            break\n",
    "        episodes += 1\n",
    "        if episodes % 100 == 0:\n",
    "            print(\"Current episode: {}\".format(episodes))\n",
    "        biggest_change = 0\n",
    "        # loop through every state present\n",
    "        for state in range(state_size):\n",
    "            old_V = V[state]\n",
    "            # take random action according to policy\n",
    "            action = policy[state]\n",
    "            new_state = state + 1\n",
    "            reward = env.values[state][action]\n",
    "            V[state] = reward + gamma * V[new_state]\n",
    "            biggest_change = max(biggest_change, abs(V[state] - old_V))\n",
    "        if biggest_change < small_change:\n",
    "            break\n",
    "            \n",
    "    # policy improvement\n",
    "    policy_changed = False\n",
    "    for state in range(state_size):\n",
    "        best_val = -np.inf\n",
    "        best_action = -1\n",
    "        for action in action_space:\n",
    "            new_state = state + 1\n",
    "            reward = env.values[state][action]\n",
    "            future_reward = reward + gamma * V[new_state]\n",
    "            if future_reward > best_val:\n",
    "                best_val = future_reward\n",
    "                best_action = action\n",
    "        assert best_action != -1\n",
    "        if policy[state] != best_action:\n",
    "            policy_changed = True\n",
    "        policy[state] = best_action\n",
    "\n",
    "    if not policy_changed:\n",
    "        break\n",
    "print(\"Total episodes trained: {}\".format(episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward collected: 10000\n"
     ]
    }
   ],
   "source": [
    "# total rewards earned\n",
    "reward = 0\n",
    "# random policy: for every state, choose a random\n",
    "# position for displaying the ad\n",
    "for x in range(len(env)):\n",
    "    action = policy[x]\n",
    "    # if the guess was correct, increase the reward\n",
    "    if env.values[x][action] == 1:\n",
    "        reward += 1\n",
    "print(\"Reward collected: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Q-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using q-learning\n",
    "states = len(env)\n",
    "actions = 10\n",
    "q_table = np.zeros((states, actions))\n",
    "\n",
    "learning_rate = 0.7\n",
    "# gamma = 0.618\n",
    "gamma = 0.9\n",
    "\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01\n",
    "max_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploit(eps):\n",
    "    \"\"\"Randomizes a number to select\n",
    "    whether or not to expolit\"\"\"\n",
    "    return np.random.uniform() > eps\n",
    "\n",
    "def random_action():\n",
    "    return np.random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 50\n"
     ]
    }
   ],
   "source": [
    "reward = 0\n",
    "for episode in range(max_episodes):\n",
    "    epsilon *= 2\n",
    "    if episode % 50 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "    for state in range(states):\n",
    "        if exploit(epsilon):\n",
    "            action = random_action()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        r = env.values[state][action]\n",
    "        reward += r\n",
    "        q_table[state][action] += learning_rate*(r + gamma*np.max(q_table[state, :]) - q_table[state][action])\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward collected: 1703\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "rewards = 0\n",
    "for state in range(states):\n",
    "    best_action = np.argmax(q_table[state, :])\n",
    "    r = env.values[state][best_action]\n",
    "    rewards += r\n",
    "print(\"Reward collected: {}\".format(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 6, ..., 4, 6, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.values.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = [np.random.randint(0, 10) for x in range(10000)]\n",
    "env = np.zeros((10000, 100))\n",
    "i = 0\n",
    "for x in rand_list:\n",
    "    env[i][x] = 1\n",
    "    i += 1\n",
    "env = pd.DataFrame(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.values[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
