{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property\n",
    "\n",
    "The Markov Property states that the probability of the future states depends only on the current state, and not any of the previous states. \n",
    "\n",
    "So p{x | x1, x2, x3....} = p{x | x1} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "Any RL task with a set of states, actions, and rewards that follows the markov property is called as a Markov Decision Process (MDP). MDP is defined by a five tuple: \n",
    "1. Set of states\n",
    "2. Set of actions\n",
    "3. Set of rewards\n",
    "4. State transition probability, reward probability\n",
    "5. Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "Policy is the algorithm that the agent uses to navigate the environment. It is denoted by $\\pi$\n",
    "\n",
    "### State transition probability\n",
    "\n",
    "$$p(s' \\ \\vert \\ s, a)$$\n",
    "\n",
    "This depicts the probability of entering a state s' given the current state s, and the action taken a. The reason it is stochastic is because we don't know what state we end up being since we have no control over the outcome. E.g: In a card playing game, we don't know what cards the other user has so we can't say for sure. \n",
    "\n",
    "### Total reward\n",
    "\n",
    "$$G\\left(t\\right)=\\sum_{i=1}^{\\infty}R\\left(t+i\\right)$$\n",
    "\n",
    "total rewards measures the rewards of all the action taken (except for the current one) \n",
    "\n",
    "### Future reward and discount factor\n",
    "\n",
    "$$G\\left(t\\right)=\\sum_{i=1}^{\\infty}\\gamma^iR\\left(t+i\\right)$$\n",
    "\n",
    "Sometimes we care more about immediate reward (greedy strategy), so \\$100 profit today is worth more than \\$150 profit next year. Since we try to maximize total rewards, we should weigh the future rewards appropriately. Discount factor indicates how much weightage must be given to the rewards in future. It is denoted by gamma. \n",
    "\n",
    "If gamma = 0 then the algorithm is truly greedy and doesn't care about future reward at all. If gamma = 1 then don't care how far in future the reward is, weigh them equally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function \n",
    "\n",
    "During the process of taking decision, we should be able to answer the following question which is of great importance: \n",
    "\"If I am in this current state s, what will the sum of total rewards G (till the end of the episode) be __on average?__ \"\n",
    "\n",
    "We call this expected value of total rewards V(s) it is denoted by: \n",
    "\n",
    "$$V(S) = \\mathbb{E}(G \\ \\vert \\ S)$$\n",
    "\n",
    "This is also called as conditional expectation. And since it accounts for only the state, it is also called as __State Value Function__ \n",
    "\n",
    "Since the value function is recursive. That is, if there were no random states, V(S) = r + V(S') we can represent it in the following way\n",
    "\n",
    "$$V\\left(S\\right)=\\mathbb{E}\\left[r+\\gamma V\\left(S'\\right)\\right]$$\n",
    "\n",
    "#### Action-Value Function\n",
    "\n",
    "An extension to the value function is by adding the current action parameter. Now it will be called the value-action function. It depicts the expected G given that we're in state s and take the action a. It's just an extension to add more granularity in the equation (since V(s) is independent of a)\n",
    "\n",
    "$$Q\\left(S,a\\right)=\\mathbb{E}\\left[r+\\gamma V\\left(S'\\right) \\ | \\ S,a\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "\n",
    "todo: derive bellman equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
