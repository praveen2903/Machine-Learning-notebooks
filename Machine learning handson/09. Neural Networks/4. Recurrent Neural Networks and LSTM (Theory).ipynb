{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Shortcomings of ANNs and CNNs\n",
    "\n",
    "Neural Nets don't have memory so each input is treated as discrete, but in many cases like sentences, the current words adds in context to the previous ones. In this case we're losing the syntactic information by using something like BOW or TF-IDF.\n",
    "\n",
    "Using RNNs we can add this feature since they add the capability of memory. They achieve this by connecting the output of previous neuron to the input of current neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://recast.ai/blog/wp-content/uploads/2017/10/recast-ai-colah-rnn-unrolled.png\">\n",
    "Source: [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of vanilla RNNs\n",
    "\n",
    "### 1. No long memory\n",
    "RNN adds memory by considering not only the current input but also the output of previous hidden state. But the problem is also exactly that. The memory of the current input is limited to the hidden state exactly before that, there are ways to elongate memory but in practise RNNs don't seem to learn them. To circumvent this issue, use LSTM\n",
    "\n",
    "### 2. Vanishing gradient\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/06210536/grad_vanishing.png\"/>\n",
    "Source: [AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)\n",
    "\n",
    "While we propagate the error during chain rule, if one of the error tends to zero, all the others will exponentially reach to zero due to the nature of multiplication. The fix is to use LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory Neural Networks\n",
    "\n",
    "LSTM have gate mechanism: input gate, output gate, and forget gate. These gates control how much information to pass in to the next layer. Steps in LSTM\n",
    "\n",
    "### 1. Foget Gate Layer\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\">\n",
    "Source: [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "$C_{t-1}$ is the previous state. The Sigmoid function will output a number between 0 to 1 to indicate the extent to which state of the previous state is allowed to flow\n",
    "\n",
    "### 2. Input Gate Layer\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\">\n",
    "Source: [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "The input layer has two parts:\n",
    "1. Sigmoid layer will decide which parts to update\n",
    "2. TanH will decide new candidate that should be added to the state\n",
    "\n",
    "### 3. Updating\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n",
    "Source: [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Previously we decided what to update, now in this step the output of forget layer (applied to output of previous state) and the current state are added. \n",
    "\n",
    "### 4. Output Layer\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n",
    "Source: [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Now this layer decide what to output and works in two steps:\n",
    "1. Sigmoid layer decides what information from currect state we are going to consider in the output. \n",
    "2. TanH of the previous state is applied and multiplied with above value to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of LSTM:\n",
    "1. Gated Recurring Units (GRU)\n",
    "This variant merges the input and forget gate to a single \"update gate\" and adds some other minor modifications. The resultant architecture is relatively simpler, and performs better than LSTM in some cases. \n",
    "\n",
    "Generally LSTM, and GRUs are nearly same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
