{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Machine Learning models sometimes fit the data too well. This causes a problem because the model won't generalize well with real world data. Consider the following\n",
    "\n",
    "<img \n",
    "src=\"https://raw.githubusercontent.com/alexeygrigorev/wiki-figures/master/ufrt/kddm/overfitting-logreg-ex.png\" \n",
    "/>\n",
    "\n",
    "The $R^2$ value, a commonly used measure to evaluate models, will clearly be high for the model since the data fits very good and the user will be tricked into believing that the mode is perfect but in reality the model is very bad and won't work with unseen data. \n",
    "\n",
    "A way to deal with the problem is to use Regularization. There are multiple ways to use Regularization, we'll see that in the following posts.  \n",
    "\n",
    "Furthermore, Regularization also helps to solve the Bais-Variance tradeoff. \n",
    "\n",
    "Source: [ML Coursera](https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjugomJysPYAhUQ4o8KHcqwCs8QFgg-MAA&url=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fmachine-learning&usg=AOvVaw2zgTKaHlTbWua1rRu2TcP9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "Norm is nothing but a way to measure distance between two vectors. Generally, it's used to measure the distance between the origin and vector. There are many forms of Norms, but for our purposes, we'll use $L_1$ and $L_2$ norm. \n",
    "\n",
    "### $L_1$ Norm or Mean Absolute Error\n",
    "\n",
    "Consider a vector x = \\[B1 B2\\] Then the $L_1$ norm is simply the addition of the two numbers inside it. \n",
    "\n",
    "Mathematically, $L_1$ Norm is denoted by the following equation\n",
    "\n",
    "### $$\\lVert x \\rVert_1=\\sum_{i=0}^n\\left|x_i\\right|$$\n",
    "\n",
    "### $L_2$ Norm or Root Mean Squared Error\n",
    "\n",
    "The only difference in the $L_2$ norm is we take the addition of the squares of all the numbers and take its square root. \n",
    "\n",
    "Mathematically, $L_2$ Norm is denoted by the following equation\n",
    "\n",
    "### $$\\lVert x \\rVert_2 = \\sqrt{\\sum_{i=0}^nx_i^2}$$\n",
    "\n",
    "\n",
    "### Example\n",
    "Consider a vector\n",
    "$$\n",
    "x =\n",
    "    \\begin{bmatrix}\n",
    "    \\beta_0 \\\\\n",
    "    \\beta_1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then $L_1$ norm will be\n",
    "$$\\lVert x \\rVert_1= \\beta_0 + \\beta_1$$\n",
    "\n",
    "And $L_2$ norm will be\n",
    "$$\\lVert x \\rVert_1= \\sqrt{\\beta_0^2+\\beta_1^2}$$\n",
    "\n",
    "Graphically, if you plot the vector considering $\\beta_0$ as x-coordinate and $\\beta_1 $as y coordinate. Then $L_1$ norm is simply the addition of the two sides of the traingles formed and $L_2$ is the hypotenuse. The drawing below will make it clear (excuse my poor handwriting :))\n",
    "\n",
    "\n",
    "<img src=\"images/vector_norm.png\" height=300 width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regulariaztion Parameter\n",
    "\n",
    "Consider the cost function of Logistic Regression from the previous post which is given by\n",
    "\n",
    "$$\n",
    "J\\left(\\theta\\right)=-\\frac{1}{m}\\sum_{i=0}^m\\left[y^{\\left(i\\right)}\\log\\left(h_{\\theta}\\left(x^{\\left(i\\right)}\\right)\\right)+\\left(1-y^{\\left(i\\right)}\\right)\\log\\left(1-h_{\\theta}\\left(x^{\\left(i\\right)}\\right)\\right)\\right]\n",
    "$$\n",
    "\n",
    "We add the term  $\\lambda \\lVert \\theta \\rVert_2 ^2$ to it, where $\\lambda$ is a constant that user gets to decide. It's also called as __Regularization Parameter__. Mathematically, it's a Lagrange multiplier that's minimising the cost function $J\\left(\\theta\\right)$.\n",
    "\n",
    "Adding the $\\lambda$ term to the equation makes the cost function for Logistic Regression as:\n",
    "\n",
    "$$J\\left(\\theta\\right)=-\\frac{1}{m}\\sum_{i=0}^m\\left[y^{\\left(i\\right)}\\log\\left(h_{\\theta}\\left(x^{\\left(i\\right)}\\right)\\right)+\\left(1-y^{\\left(i\\right)}\\right)\\log\\left(1-h_{\\theta}\\left(x^{\\left(i\\right)}\\right)\\right)\\right] + \n",
    "\\lambda \\lVert \\theta \\rVert_2 ^2$$\n",
    "\n",
    "Note the notation we used, the 2 at the bottom of $\\theta$ indicates that's a $L_2$ norm, hence, technically we're using Ridge Regression (We'll see shortly what it is). Normally this piece of notation is ignored since whenever we indicate norm we're almost always using the $L_2$ norm and not the $L_1$ one. But we'll be explicit about it since it's the subtle difference between Ridge and Lasso Regression. \n",
    "\n",
    "#### Note:\n",
    "If $\\lambda$ is:\n",
    "1. __too high__: The values will be heavily penalized and __underfitting__ will occur. \n",
    "2. __too low__: There'll be no regularization, we'll be where we started from and the model will __overfit__. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Regularization\n",
    "\n",
    "Two types of Regularization we'll see are Ridge, and LASSO. \n",
    "\n",
    "### 1. Ridge\n",
    "Notice how in the above equation we used $L_2$ norm. That's the key point of Ridge Regularization. \n",
    "\n",
    "### 2. LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "Here instead of the $L_2$ norm, we take $L_1$ norm. This type of regularization is rarely used. \n",
    "\n",
    "### 3. ElasticNet\n",
    "This combines both Ridge and LASSO hence uses both $L_1$ and $L_2$ norms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use $L_1$ (MAE) vs $L_2$ (RMSE) \n",
    "\n",
    "Since RMSE squares the differences this has the effect of penalizing large errors more than lower errors. For instance penalizing an error of 10 is __more than twice__ than penalizing the error of 5. \n",
    "\n",
    "But in the case of MAE penalizing an error of 10 is __twice__ the penalizing the error of 5. Hence whenever you want to penalizing linearly you ought to choose MAE over RMSE else the other way around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
