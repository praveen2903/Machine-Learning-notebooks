{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## AutoEncoders\n",
    "\n",
    "Autoencoders are used to transform your data from one form (less efficient) to a better representation (more efficient) This practise is useful for pre-training models. Precisely, when you train your NN, you initialise your weights randomly according to gaussian distribution. But in the end, they're random weights you initialise with are often inefficient and tend to find local minima. Instead of feeding the network with input, the input is used to train the autoencoder which is better representation of the same data, which is then used to feed the model. \n",
    "\n",
    "<img src=\"https://probablydance.files.wordpress.com/2016/04/3_middle_layer.png\"/>\n",
    "Source: [Probably Dance](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef6a2885af41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named torch"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                   1                             2\n",
      "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4  5  Father of the Bride Part II (1995)                        Comedy\n",
      "   0  1   2   3      4\n",
      "0  1  F   1  10  48067\n",
      "1  2  M  56  16  70072\n",
      "2  3  M  25  15  55117\n",
      "3  4  M  45   7  02460\n",
      "4  5  M  25  20  55455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::', \n",
    "                      header=None, engine='python',\n",
    "                      encoding='latin-1')\n",
    "print movies.head()\n",
    "\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::', \n",
    "                      header=None, engine='python',\n",
    "                      encoding='latin-1')\n",
    "print users.head()\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', \n",
    "                      header=None, engine='python',\n",
    "                      encoding='latin-1')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', \n",
    "                           delimiter='\\t').values \n",
    "test_set = pd.read_csv('ml-100k/u1.test', \n",
    "                       delimiter='\\t').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the number of users and movies \n",
    "nb_users = int(max(max(training_set[:, 0]),\n",
    "                   max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]),\n",
    "                   max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the data into an an array with users \n",
    "# in lines and movies in columns.\n",
    "# This is because torch expects the data to be\n",
    "# in this fashion\n",
    "def convert(data):\n",
    "    \"\"\"Create a list of lists. One list for each user. \n",
    "    The rating are from 1 to 5. User that didn't see \n",
    "    the movie will have a rating of 0\"\"\"\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        # take the column with movie ids for this\n",
    "        # specific user\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users] \n",
    "        # same for the ratings\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        # we also have to take care of the case where\n",
    "        # this user didn't watch a specific movie\n",
    "        # So, create a list of 1682 (total movies) elements\n",
    "        # initialized with 0 and set the rating where this\n",
    "        # person has watched the movies. \n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the data into torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For autoencoders we are predicting the ratings also\n",
    "# so no need to change the input to binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the architecture of Auto Encoder\n",
    "# inherit from nn.Module to inhert functions\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    \"\"\"Creates a Stacked Auto Encoder\n",
    "    Configuration:\n",
    "    Input -> 20 -> 10 -> 20 -> Input\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "        # first layer that will take input nb_movies\n",
    "        # the number of nodes in hidden layer will be 20\n",
    "        self.first_full_connection = nn.Linear(nb_movies, 20)\n",
    "        # make second full connection\n",
    "        self.second_full_connection = nn.Linear(20, 10)\n",
    "        # make third full connection\n",
    "        self.third_full_connection = nn.Linear(10, 20)\n",
    "        # make fourth full connection\n",
    "        self.fourth_ful_connection = nn.Linear(20, nb_movies)\n",
    "        # use sigmoid activation function\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_vector):\n",
    "        \"\"\"\n",
    "        Encode and perform forward pass\n",
    "        \"\"\"\n",
    "        input_vector = self.activation(\n",
    "                        self.first_full_connection(input_vector))\n",
    "        input_vector = self.activation(\n",
    "                        self.second_full_connection(input_vector))\n",
    "        input_vector = self.activation(\n",
    "                        self.third_full_connection(input_vector))\n",
    "        # no activation required for the last layer\n",
    "        input_vector = self.fourth_ful_connection(input_vector)\n",
    "        return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_auto_encoder = StackedAutoEncoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(stacked_auto_encoder.parameters(), lr=0.01,\n",
    "                         weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 1.91791504736\n",
      "Epoch: 2\tTraining Loss: 1.2089137057\n",
      "Epoch: 3\tTraining Loss: 1.16117812748\n",
      "Epoch: 4\tTraining Loss: 1.1449053598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c9949e6ea124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             mean_corrector = nb_movies /                                 float(torch.sum(target_variable > 0)\n\u001b[1;32m     28\u001b[0m                                       + 1e-10)\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# take squared loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmean_corrector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/uzumaki/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/uzumaki/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the Stacked Auto Encoder\n",
    "number_of_epochs = 200\n",
    "for epoch in range(number_of_epochs):\n",
    "    training_loss = float(0)\n",
    "    counter = float(0)\n",
    "\n",
    "    for id_user in range(nb_users):\n",
    "        # Variable is used to create the same effect\n",
    "        # as np.reshape(-1, 1)\n",
    "        input_vector = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        # creates copy\n",
    "        target_variable = input_vector.clone()\n",
    "        # exclude users who didn't rate any movies\n",
    "        if torch.sum(target_variable.data > 0) > 0:\n",
    "            # just pass it to the constructor\n",
    "            output = stacked_auto_encoder(input_vector)\n",
    "            # don't compute gradients of the target\n",
    "            # this saves computational cost\n",
    "            target_variable.require_grad = False\n",
    "            # reset the output of the variables\n",
    "            # where the user hasn't rated any rating\n",
    "            output[target_variable==0] = 0\n",
    "            loss = criterion(output, target_variable)\n",
    "            # This is needed because we want to calculate\n",
    "            # the error on only those movies who have \n",
    "            # ratings\n",
    "            mean_corrector = nb_movies / \\\n",
    "                                float(torch.sum(target_variable > 0)\n",
    "                                      + 1e-10)\n",
    "            loss.backward()\n",
    "            # take squared loss\n",
    "            training_loss +=  np.sqrt(loss.data[0] * mean_corrector)\n",
    "            counter += 1\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "    print 'Epoch: {0}\\tTraining Loss: {1}'.format(epoch + 1, \n",
    "                                                  training_loss/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTrest Loss: 0.0\n",
      "Epoch: 2\tTrest Loss: 0.0\n",
      "Epoch: 3\tTrest Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-830c30f6a4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             mean_corrector = nb_movies /                                 float(torch.sum(target_variable > 0)\n\u001b[1;32m     28\u001b[0m                                       + 1e-10)\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# take squared loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmean_corrector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/uzumaki/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/uzumaki/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing the Stacked Auto Encoder\n",
    "test_loss = 0\n",
    "counter = 0\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    # Variable is used to create the same effect\n",
    "    # as np.reshape(-1, 1)\n",
    "    input_vector = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target_variable = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    # exclude users who didn't rate any movies\n",
    "    if torch.sum(target_variable.data > 0) > 0:\n",
    "        # just pass it to the constructor\n",
    "        output = stacked_auto_encoder(input_vector)\n",
    "        # don't compute gradients of the target\n",
    "        # this saves computational cost\n",
    "        target_variable.require_grad = False\n",
    "        output[target_variable == 0] = 0\n",
    "        loss = criterion(output, target_variable)\n",
    "        # This is needed because we want to calculate\n",
    "        # the error on only those movies who have \n",
    "        # ratings\n",
    "        mean_corrector = nb_movies / float(torch.sum(\n",
    "                                                target_variable > 0)\n",
    "                                           + 1e-10)\n",
    "        # take mean squared loss\n",
    "        training_loss +=  np.sqrt(loss.data[0] * mean_corrector)\n",
    "        counter += 1\n",
    "print 'Test Loss: {1}'.format(epoch + 1, test_loss/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
