{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzmann Machine\n",
    "Boltzmann machine's learns the internal concepts (not defined by the user) that help to explain the observed data. This is achieved by the hidden units that are connected to the input. Commonly used variant of Boltzmann Machine is the Restricted Boltzmann Machine's where the hidden units are not connected amongst themselves (this gives a strong performance boost) \n",
    "\n",
    "Boltzmann Machines try to model the distribution of the data using binary hidden neurons. Furthermore, Boltzmann Machines are Unsupervised Stochastic Neural Networks (stochastic means they have an element of probability in them). \n",
    "\n",
    "The basic idea of Boltzmann Machine is drawn from concept from Physics which is Energy. Every state is given an energy, and the most likely state will have the least energy. Boltzmann Machine uses Energy function to calculate the probability of each state. The one that has highest probability is the state chosen by the system. The learning is done by the weights that are given to the connections. \n",
    "\n",
    "Architecture of Boltzmann Machine is as follows:\n",
    "1. One visible layer: Where the input goes (like the users' movie ratings) \n",
    "2. One hidden layer: Which calculates these latent features not defined by user\n",
    "3. Bias: A way to adjust the ratings of movie \n",
    "\n",
    "Visible layers are connected to hidden layers, but the they are not connected amongst themselves. That is, no visible unit is connect to another visible unit, and no hidden unit is connected to another hidden unit. The bias, on the other hand is connected to both, hidden and visible unit. \n",
    "\n",
    "\n",
    "### Learning Process\n",
    "The learning process of RBMs is using a process called **Contrastive Divergence** which is approximately same as the process of gradient descent. \n",
    "\n",
    "Intuitive Process:\n",
    "1. The input at visible node is multiplied by the weights at visible node and passed through activation function which will give the probability of activation of a hidden node. \n",
    "2. Now based on the activation of hidden node, the probability of the activation of visible node is calculated which tries to model the same input that it received first. \n",
    "3. Perform random walk for k times. This is the Contrastive Divergence process. \n",
    "3. At the end of the k steps, if there's an error, for instance, the input[0] was 1 and the output is 0, the error is backpropagated and the weights, and biases are adjusted accordingly\n",
    "\n",
    "\n",
    "Mathematically:\n",
    "1. Activation of a hidden unit is given by: \n",
    "$$a = \\sum_i{w_{ij}x_i + b}$$\n",
    "and the probability is $$p(x) = \\sigma(a)$$\n",
    "where, $\\sigma(x)$ is the sigmoid activation function\n",
    "\n",
    "2. TODO: The derivation of energy function and Contrastive Divergence\n",
    "\n",
    "Variation of RBN is the Deep Belief Network. Deep Belief Network is just a stacked RBN. Another variation is Deep Boltzmann Machine which is nearly the same as DBN but the difference is, in DBN all the layers except the last 2 are directed downwards, in DBM there are no such limitations\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-0f880856d4d1e886bda98ba2b292e6aa\">\n",
    "\n",
    "Useful links:\n",
    "1. [Detailed algorithm paper by Geoff Hinton](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n",
    "2. [Intuitive understanding of the algorithm](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)\n",
    "2. [Derivation of Contrastive Divergence](http://image.diku.dk/igel/paper/AItRBM-proof.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                   1                             2\n",
      "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4  5  Father of the Bride Part II (1995)                        Comedy\n",
      "   0  1   2   3      4\n",
      "0  1  F   1  10  48067\n",
      "1  2  M  56  16  70072\n",
      "2  3  M  25  15  55117\n",
      "3  4  M  45   7  02460\n",
      "4  5  M  25  20  55455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::', \n",
    "                      header=None, engine='python',\n",
    "                      encoding='latin-1')\n",
    "print movies.head()\n",
    "\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::', \n",
    "                      header=None, engine='python',\n",
    "                      encoding='latin-1')\n",
    "print users.head()\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', \n",
    "                      header=None, engine='python',\n",
    "                      encoding='latin-1')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', \n",
    "                           delimiter='\\t').values \n",
    "test_set = pd.read_csv('ml-100k/u1.test', \n",
    "                       delimiter='\\t').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the number of users and movies \n",
    "nb_users = int(max(max(training_set[:, 0]),\n",
    "                   max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]),\n",
    "                   max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the data into an an array with users \n",
    "# in lines and movies in columns.\n",
    "# This is because torch expects the data to be\n",
    "# in this fashion\n",
    "def convert(data):\n",
    "    \"\"\"Create a list of lists. One list for each user. \n",
    "    The rating are from 1 to 5. User that didn't see \n",
    "    the movie will have a rating of 0\"\"\"\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        # take the column with movie ids for this\n",
    "        # specific user\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users] \n",
    "        # same for the ratings\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        # we also have to take care of the case where\n",
    "        # this user didn't watch a specific movie\n",
    "        # So, create a list of 1682 (total movies) elements\n",
    "        # initialized with 0 and set the rating where this\n",
    "        # person has watched the movies. \n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the data into torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the ratings into binary 1 (liked) or 0 (not liked)\n",
    "# we have to do this because the output of RBM will be \n",
    "# 0 if it predicts user will like the movie and 1 if the \n",
    "# prediction is yes. \n",
    "\n",
    "# ratings will be modified to:\n",
    "# 1. Not watched                   -1\n",
    "# 2. Ratings 3 and above (liked)    1\n",
    "# 3. Ratings 1 or 2 (disliked)      0\n",
    "\n",
    "# convert 0 ratings to -1 \n",
    "training_set[training_set == 0] = -1\n",
    "\n",
    "# convert ratings for 1 and 2 to 0\n",
    "# using training_set[training_set <= 2] = 0\n",
    "# has inadvertent effect on rows with rating 0\n",
    "\n",
    "training_set[training_set == 1] = 0\n",
    "training_set[training_set == 2] = 0\n",
    "\n",
    "# convert ratings 3 and above to 1\n",
    "training_set[training_set >= 3] = 1\n",
    "\n",
    "# same for test set\n",
    "test_set[test_set == 0] = -1\n",
    "\n",
    "test_set[test_set == 1] = 0\n",
    "test_set[test_set == 2] = 0\n",
    "\n",
    "# convert ratings 3 and above to 1\n",
    "test_set[test_set >= 3] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the RBM architecture\n",
    "\n",
    "class RBM(object):\n",
    "    \"\"\" Creates a Restricted Boltzmann Machine\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        1. hidden :  number of hidden nodes in RBM\n",
    "        2. visible : number of visible nodes in RBM\n",
    "        \"\"\"\n",
    "    def __init__(self, visible, hidden):\n",
    "        # The weights matrix is the weight of the path connected\n",
    "        # from visible node i to hidden node j. Every path\n",
    "        # will have a certain weight. \n",
    "        \n",
    "        # initialise the weights according to normal distribution \n",
    "        # which has mean of 0 and variance of 1 dimensions will \n",
    "        # be hidden x weights. \n",
    "        self.weights = torch.randn(hidden, visible)\n",
    "        # bias_h is the bias of hidden nodes which is required \n",
    "        # to calculate the probability of hidden node given \n",
    "        # visible node\n",
    "        self.bias_h = torch.randn(1, hidden)\n",
    "        # bias_v is the bias of visible nodes which is required\n",
    "        # to calculate the probability of visible node given \n",
    "        # visible node\n",
    "        self.bias_v = torch.randn(1, visible)\n",
    "    \n",
    "    def activate_hidden(self, visible):\n",
    "        \"\"\"This will activate some hidden nodes using the probability \n",
    "        of hidden given visible or p(h=1 | v) which is nothing but\n",
    "        sigmoid applied to the sum(weights*input + bias). \n",
    "        The question then we'll ask is which hidden units will \n",
    "        activate given these active visible nodes. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        1. visible :    These are visible units in p(h=1 | v) or\n",
    "                        the current visible nodes that are active.\n",
    "                   \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        1. p_h_given_v : Returns the vector (one value for every \n",
    "                         hidden node) of float values.\n",
    "        \n",
    "        2. activated_values : The nodes will be activated according to\n",
    "                         bernoulli's sampling. Which basically works\n",
    "                         as follows: If you have a probability of 0.7\n",
    "                         Then choose a random number between 0 and 1\n",
    "                         if the number is less than 0.7, activate the\n",
    "                         node, else leave it off. \n",
    "           \n",
    "        \"\"\"\n",
    "        weight_x = torch.mm(visible, self.weights.t())\n",
    "        # expand as will add a dimension so that bias is applied\n",
    "        # to each line of the weight_x\n",
    "        activation = weight_x + self.bias_h.expand_as(weight_x)\n",
    "        # calculate p(h | v)\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
    "    \n",
    "    def activate_visible(self, hidden):\n",
    "        \"\"\"This will activate visible nodes using the probability \n",
    "        of visible given hidden or p(v=1 | h) which is nothing but\n",
    "        sigmoid applied to the sum(weights*input + bias). \n",
    "        The question then we'll ask is which visible units will \n",
    "        activate given these hidden nodes. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        1. hidden :      These are hidden units in p(v=1 | h) or\n",
    "                         the current hidden nodes that are active.\n",
    "                   \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        1. p_v_given_h : Returns the vector (one value for every \n",
    "                         hidden node) of float values.\n",
    "        \n",
    "        2. activated_values : The nodes will be activated according to\n",
    "                         bernoulli's sampling. Which basically works\n",
    "                         as follows: If you have a probability of 0.7\n",
    "                         Then choose a random number between 0 and 1\n",
    "                         if the number is less than 0.7, activate the\n",
    "                         node, else leave it off. \n",
    "           \n",
    "        \"\"\"\n",
    "        # This time we don't need to compute the transpose \n",
    "        # since we're going the other way\n",
    "        weight_x = torch.mm(hidden, self.weights)\n",
    "        # expand as will add a dimension so that bias is applied\n",
    "        # to each line of the weight_x\n",
    "        activation = weight_x + self.bias_v.expand_as(weight_x)\n",
    "        # calculate p(v | h)\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "    \n",
    "    def train(self, input_vector, predicted_visible_k, \n",
    "              p_h_given_v_0, p_h_given_v_k):\n",
    "        \"\"\"\n",
    "        Performs Contrastive Divergence\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        1. input_vector :  The original input vector given by the user.\n",
    "        \n",
    "        2. predicted_visible_k : The visible vector obtained after k\n",
    "                           iterations in Contrastive Divergence. \n",
    "        \n",
    "        3. p_h_given_v_0 : The probabilities of hidden nodes given the\n",
    "                           input vector. \n",
    "        \n",
    "        4. p_h_given_v_k : The probabilities of hidden nodes at the kth\n",
    "                           iteration in Contrastive Divergence. \n",
    "        \n",
    "        \"\"\"\n",
    "        # update weights\n",
    "        self.weights += torch.mm(input_vector.t(), p_h_given_v_0) \\\n",
    "                            - torch.mm(predicted_visible_k.t(), \n",
    "                                       p_h_given_v_k)\n",
    "        # the zero in the end of sum() is to preserve the shape\n",
    "        # as matrix. \n",
    "        self.bias_v += torch.sum((input_vector - predicted_visible_k), 0)\n",
    "        self.bias_h += torch.sum((p_h_given_v_0 - p_h_given_v_k), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 0.292761849616\n",
      "Epoch: 2\tTraining Loss: 0.251646004149\n",
      "Epoch: 3\tTraining Loss: 0.252478372312\n",
      "Epoch: 4\tTraining Loss: 0.250084695766\n",
      "Epoch: 5\tTraining Loss: 0.250953539255\n",
      "Epoch: 6\tTraining Loss: 0.249900776658\n",
      "Epoch: 7\tTraining Loss: 0.251415914688\n",
      "Epoch: 8\tTraining Loss: 0.250097338458\n",
      "Epoch: 9\tTraining Loss: 0.249537683653\n",
      "Epoch: 10\tTraining Loss: 0.248545808809\n",
      "Epoch: 11\tTraining Loss: 0.249935396027\n",
      "Epoch: 12\tTraining Loss: 0.250694418432\n",
      "Epoch: 13\tTraining Loss: 0.253130697998\n",
      "Epoch: 14\tTraining Loss: 0.249886217562\n",
      "Epoch: 15\tTraining Loss: 0.250107781537\n"
     ]
    }
   ],
   "source": [
    "visible_nodes_size = len(training_set[0])\n",
    "hidden_nodes_size = 100\n",
    "batch_size = 100\n",
    "number_of_epochs = 15\n",
    "rbm = RBM(visible_nodes_size, hidden_nodes_size)\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    train_loss = float(0)\n",
    "    counter = float(0)\n",
    "    for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "        # visible_k is the configuration of visible nodes after\n",
    "        # k iterations.\n",
    "        # initially, it is the same as the input vector. \n",
    "        visible_k = training_set[id_user: id_user + batch_size]\n",
    "        visible_0 = training_set[id_user: id_user + batch_size]\n",
    "        p_h_given_v0, _ = rbm.activate_hidden(visible_0)\n",
    "        # perform k steps of Contrastive Divergence\n",
    "        for k in range(10):\n",
    "            # perform random walk\n",
    "            \n",
    "            # hidden_k are the probabilities of hideen nodes\n",
    "            # after k passes\n",
    "            \n",
    "            # initially visible_k = visible_0\n",
    "            _, hidden_k = rbm.activate_hidden(visible_k)\n",
    "            _, visible_k = rbm.activate_visible(hidden_k)\n",
    "            # reset the nodes where the rating is -1\n",
    "            # i.e. where the users didn't watch the movie. \n",
    "            visible_k[visible_0 < 0] = visible_0[visible_0 < 0]\n",
    "        p_h_given_vk, _ = rbm.activate_hidden(visible_k)\n",
    "        rbm.train(visible_0, visible_k, p_h_given_v0, p_h_given_vk)\n",
    "        # don't measure the error where the ratings are -1\n",
    "        train_loss += torch.mean(torch.abs(visible_0[visible_0 >= 0]\n",
    "                                           - visible_k[visible_0 >= 0]))\n",
    "        counter += 1\n",
    "    print 'Epoch: {0}\\tTraining Loss: {1}'.format(epoch + 1, \n",
    "                                                 train_loss/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.229938048396\n"
     ]
    }
   ],
   "source": [
    "# Testing the predictions\n",
    "\n",
    "test_loss = float(0)\n",
    "counter = float(0)\n",
    "for id_user in range(nb_users):\n",
    "    # IMPORTANT: While testing, we start with the\n",
    "    # trainin_set data for this user, which will\n",
    "    # be used to predict the recommendations for\n",
    "    # this user\n",
    "    visible_predict = training_set[id_user: id_user + 1]\n",
    "    # the test set contains actual movies watched\n",
    "    # by the same person, if our predictions match\n",
    "    # the actual results, then we're doing good. \n",
    "    visible_target = test_set[id_user: id_user + 1]\n",
    "    # We have already performed k steps of Contrastive\n",
    "    # Divergence, so while testing we need to do it\n",
    "    # only once. \n",
    "    # Also, we cannot predict when the user hasn't\n",
    "    # watched any movies, so keep that case in mind. \n",
    "    if len(visible_target[visible_target >= 0]) > 0:\n",
    "        _, hidden_predict = rbm.activate_hidden(visible)\n",
    "        _, visible_predict = rbm.activate_visible(hidden_predict)\n",
    "        # don't measure the error where the ratings are -1\n",
    "        test_loss += torch.mean(torch.abs(\n",
    "                                visible_target[visible_target >= 0]\n",
    "                                - visible_predict[visible_target >= 0]))\n",
    "        counter += 1\n",
    "print 'Test Loss: {1}'.format(epoch + 1, test_loss/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
